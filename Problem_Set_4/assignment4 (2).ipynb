{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3be4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from numpy.linalg import inv, solve\n",
    "from scipy.stats import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305bdead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Food   Beer  Smoke  Games  Books  Hshld  Clths  Hlth  Chems  \\\n",
      "date                                                                      \n",
      "1926-07-01  0.34  -5.41   1.07   2.71  10.75  -0.70   7.86  1.55   7.92   \n",
      "1926-08-01  2.34  26.78   6.25   0.30   9.76  -3.83  -2.76  4.00   5.25   \n",
      "1926-09-01  0.93   3.79   1.03   6.35  -1.22   0.50  -0.74  0.46   5.10   \n",
      "1926-10-01 -3.38  -3.63   0.74  -5.08   9.15  -5.00  -0.20 -0.89  -5.08   \n",
      "1926-11-01  6.04   6.98   4.24   1.35  -6.11  -0.85   1.56  5.11   4.89   \n",
      "\n",
      "            Txtls  ...  Telcm  Servs  BusEq  Paper  Trans  Whlsl  Rtail  \\\n",
      "date               ...                                                    \n",
      "1926-07-01   0.17  ...   0.61   9.00   1.84   7.48   1.71 -24.01  -0.15   \n",
      "1926-08-01   7.89  ...   1.92   1.77   4.14  -2.63   4.63   5.14  -1.00   \n",
      "1926-09-01   2.08  ...   2.18   2.02  -0.04  -5.77  -0.18  -8.10   0.02   \n",
      "1926-10-01   0.68  ...  -0.43  -2.32  -1.41  -5.40  -2.96 -15.70  -2.52   \n",
      "1926-11-01   2.80  ...   1.32   3.46   3.33   3.53   1.29   4.36   6.21   \n",
      "\n",
      "            Meals   Fin  Other  \n",
      "date                            \n",
      "1926-07-01   1.65  0.15   4.98  \n",
      "1926-08-01  -0.38  4.21   6.51  \n",
      "1926-09-01  -0.79 -1.46  -4.09  \n",
      "1926-10-01  -4.43 -5.48  -8.81  \n",
      "1926-11-01   4.02  1.93   3.69  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "            Loser   2   3   4   5   6   7   8   9  Winner\n",
      "date                                                     \n",
      "1926-07-01    NaN NaN NaN NaN NaN NaN NaN NaN NaN     NaN\n",
      "1926-08-01    NaN NaN NaN NaN NaN NaN NaN NaN NaN     NaN\n",
      "1926-09-01    NaN NaN NaN NaN NaN NaN NaN NaN NaN     NaN\n",
      "1926-10-01    NaN NaN NaN NaN NaN NaN NaN NaN NaN     NaN\n",
      "1926-11-01    NaN NaN NaN NaN NaN NaN NaN NaN NaN     NaN\n",
      "\n",
      "            Small_Low  Small_2  Small_3  Small_4  Small_High  2_Low   2_2  \\\n",
      "date                                                                        \n",
      "1926-07-01       3.56    -0.63    -2.16     0.13        1.83   1.97  2.20   \n",
      "1926-08-01      -2.46    -8.98     2.19     0.36        8.15   1.92 -1.43   \n",
      "1926-09-01      -6.44    -0.53    -6.43    -1.87        0.63  -2.08 -1.49   \n",
      "1926-10-01      -8.94    -4.07    -5.99     5.40       -2.87  -2.12 -3.59   \n",
      "1926-11-01       3.16     6.34     1.95    -5.01        0.23   2.60 -2.68   \n",
      "\n",
      "             2_3   2_4  2_High  ...  4_Low   4_2   4_3   4_4  4_High  Big_Low  \\\n",
      "date                            ...                                             \n",
      "1926-07-01  0.27 -1.80   -0.55  ...   1.37  1.31  0.97  0.05    2.25     3.23   \n",
      "1926-08-01  3.76  0.21    5.92  ...   1.08  3.62  1.76  1.92    5.09     0.76   \n",
      "1926-09-01  0.85 -3.27   -1.17  ...   0.86 -0.76 -1.96  1.23    0.64    -1.52   \n",
      "1926-10-01 -5.39 -8.37   -1.64  ...  -3.66 -2.98 -2.35 -3.43   -5.67    -3.06   \n",
      "1926-11-01  2.70  4.35    2.42  ...   3.13  2.08  3.43  4.62    1.51     3.98   \n",
      "\n",
      "            Big_2  Big_3  Big_4  Big_High  \n",
      "date                                       \n",
      "1926-07-01   5.87   1.81   2.89      0.34  \n",
      "1926-08-01   3.94   1.76   5.23      7.51  \n",
      "1926-09-01   3.42  -0.13  -0.98     -2.66  \n",
      "1926-10-01  -3.33  -2.56  -4.99     -6.13  \n",
      "1926-11-01   2.22   1.21   3.35      2.25  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "## Data loading \n",
    "\n",
    "def load_data():\n",
    "    vw_industries = pd.read_excel('../data/Problem_Set4.xlsx', sheet_name = 'Industry portfolios (VW)', header=2)\n",
    "    vw_industries = vw_industries.loc[:, :'Other']\n",
    "    pr_portfolios = pd.read_excel('../data/Problem_Set4.xlsx', sheet_name='Past return portfolios', header=8)\n",
    "    beme_portfolios = pd.read_excel('../data/Problem_Set4.xlsx', sheet_name='25 size and BEME portfolios', header=[1, 2])\n",
    "    factors = pd.read_excel('../data/Problem_Set4.xlsx', sheet_name='Market, Rf', header =1)\n",
    "    \n",
    "    new_cols = []\n",
    "    for col in beme_portfolios.columns:\n",
    "        if 'Unnamed' in str(col[0]):\n",
    "            new_cols.append('Date')\n",
    "        else:\n",
    "            new_cols.append(f\"{col[0]}_{col[1]}\")\n",
    "    beme_portfolios.columns = new_cols\n",
    "\n",
    "    return vw_industries, pr_portfolios, beme_portfolios, factors\n",
    "\n",
    "vw_industries, pr_portfolios, beme_portfolios ,factors = load_data()\n",
    "\n",
    "# Clean the data: \n",
    "def clean_data(df, replacement):\n",
    "    df = df.replace([-99.99, -999], replacement)\n",
    "    df['date'] = pd.to_datetime(df.iloc[:,0], format='%Y%m')\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df.iloc[:,1:]\n",
    "    return df\n",
    "\n",
    "vw_industries = clean_data(vw_industries,  np.nan)\n",
    "pr_portfolios = clean_data(pr_portfolios, np.nan)\n",
    "beme_portfolios = clean_data(beme_portfolios, np.nan)\n",
    "factors = clean_data(factors, np.nan)\n",
    "\n",
    "def excess_returns(df, factors):\n",
    "    df = df.subtract(factors['RF'], axis=0)\n",
    "    return df\n",
    "\n",
    "vw_industries = excess_returns(vw_industries, factors)\n",
    "pr_portfolios = excess_returns(pr_portfolios, factors)\n",
    "beme_portfolios = excess_returns(beme_portfolios, factors)\n",
    "\n",
    "print(vw_industries.head(5), pr_portfolios.head(5), beme_portfolios.head(5), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b065511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotes on TA session: \\n- Problem B, just follow the formulas and calculate the GRS statistic => GRS is testing whether a portfolio is mean variance efficient. Which means \\nthat there is no other portfolio that has a higher return for the same level of risk.\\n- Problem D => economic significance and statistical signifcance (\"alpha of 5% but p values is very large 60%, so we reject the null hypothesis at 5% level but \\nthe magnitude is still very high\"). Remember if alpha is positive then there some return that is not explained by the risk factors. \\n- Part B, we just changed the dataset \\n- Part C => \"How is it different to last weeks assignment? \\n    - \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Notes on TA session: \n",
    "- Problem B, just follow the formulas and calculate the GRS statistic => GRS is testing whether a portfolio is mean variance efficient. Which means \n",
    "that there is no other portfolio that has a higher return for the same level of risk.\n",
    "- Problem D => economic significance and statistical signifcance (\"alpha of 5% but p values is very large 60%, so we reject the null hypothesis at 5% level but \n",
    "the magnitude is still very high\"). Remember if alpha is positive then there some return that is not explained by the risk factors. \n",
    "- Part B, we just changed the dataset \n",
    "- Part C => \"How is it different to last weeks assignment? \n",
    "    - \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f05e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1\n",
    "## Question A \n",
    "''' Lets calculate the sample mean and sample standard deviation for each of the 30 portfolios.'''\n",
    "results = []\n",
    "\n",
    "def summary_stats(df):\n",
    "    for columns in df.columns: \n",
    "        mean = df[columns].mean()\n",
    "        std = df[columns].std()\n",
    "        sharpe_ratio = mean / std if std != 0 else 0\n",
    "        results.append((columns, mean, std, sharpe_ratio))\n",
    "    df_results = pd.DataFrame(results, columns= ['Portfolio', 'Mean', 'Std', 'Sharpe Ratio'])\n",
    "    df_results.set_index('Portfolio', inplace=True)\n",
    "    overall_mean = df_results['Mean'].mean()\n",
    "    overall_std = df_results['Std'].mean()\n",
    "    overall_sharpe = overall_mean / overall_std if overall_std != 0 else 0\n",
    "    df_results.loc['Overall'] = [overall_mean, overall_std, overall_sharpe]\n",
    "    return df_results\n",
    "\n",
    "summary_stats = summary_stats(vw_industries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e65eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Alpha      Beta  \\\n",
      "Portfolio                       \n",
      "Food       0.218167  0.739370   \n",
      "Beer       0.317082  0.942062   \n",
      "Smoke      0.482244  0.628665   \n",
      "Games     -0.059115  1.388648   \n",
      "Books     -0.076998  1.107988   \n",
      "Hshld      0.057837  0.902271   \n",
      "Clths      0.117064  0.813177   \n",
      "Hlth       0.255263  0.839263   \n",
      "Chems      0.103032  1.042579   \n",
      "Txtls     -0.017285  1.138909   \n",
      "Cnstr     -0.102418  1.172375   \n",
      "Steel     -0.243096  1.357006   \n",
      "FabPr     -0.035984  1.241626   \n",
      "ElcEq      0.057924  1.284064   \n",
      "Autos     -0.012242  1.252084   \n",
      "Carry      0.073151  1.188701   \n",
      "Mines      0.042680  0.908179   \n",
      "Coal      -0.050477  1.299820   \n",
      "Oil        0.185034  0.868141   \n",
      "Util       0.093951  0.778292   \n",
      "Telcm      0.153157  0.661480   \n",
      "Servs      0.397597  0.812840   \n",
      "BusEq      0.135660  1.077502   \n",
      "Paper      0.132112  0.954503   \n",
      "Trans     -0.091217  1.137771   \n",
      "Whlsl     -0.158732  1.089085   \n",
      "Rtail      0.120500  0.965016   \n",
      "Meals      0.163777  0.945508   \n",
      "Fin       -0.019294  1.166701   \n",
      "Other     -0.167665  1.064848   \n",
      "\n",
      "                                                   Residuals  \n",
      "Portfolio                                                     \n",
      "Food       date\n",
      "1926-07-01   -2.066704\n",
      "1926-08-01    0.16...  \n",
      "Beer       date\n",
      "1926-07-01    -8.515584\n",
      "1926-08-01    23....  \n",
      "Smoke      date\n",
      "1926-07-01   -1.273093\n",
      "1926-08-01    4.10...  \n",
      "Games      date\n",
      "1926-07-01   -1.341283\n",
      "1926-08-01   -3.30...  \n",
      "Books      date\n",
      "1926-07-01     7.547354\n",
      "1926-08-01     6....  \n",
      "Hshld      date\n",
      "1926-07-01   -3.428560\n",
      "1926-08-01   -6.26...  \n",
      "Clths      date\n",
      "1926-07-01    5.335933\n",
      "1926-08-01   -5.02...  \n",
      "Hlth       date\n",
      "1926-07-01   -1.189480\n",
      "1926-08-01    1.52...  \n",
      "Chems      date\n",
      "1926-07-01    4.730933\n",
      "1926-08-01    2.39...  \n",
      "Txtls      date\n",
      "1926-07-01   -3.183886\n",
      "1926-08-01    4.90...  \n",
      "Cnstr      date\n",
      "1926-07-01   -1.517811\n",
      "1926-08-01    1.47...  \n",
      "Steel      date\n",
      "1926-07-01     0.076358\n",
      "1926-08-01    -1....  \n",
      "FabPr      date\n",
      "1926-07-01    1.570770\n",
      "1926-08-01   -1.43...  \n",
      "ElcEq      date\n",
      "1926-07-01   -0.898754\n",
      "1926-08-01   -1.59...  \n",
      "Autos      date\n",
      "1926-07-01    12.476073\n",
      "1926-08-01     0....  \n",
      "Carry      date\n",
      "1926-07-01   -2.791706\n",
      "1926-08-01   -1.80...  \n",
      "Mines      date\n",
      "1926-07-01     2.689110\n",
      "1926-08-01    -2....  \n",
      "Coal       date\n",
      "1926-07-01    -2.476992\n",
      "1926-08-01    -2....  \n",
      "Oil        date\n",
      "1926-07-01   -4.374732\n",
      "1926-08-01    0.96...  \n",
      "Util       date\n",
      "1926-07-01    4.422305\n",
      "1926-08-01   -4.08...  \n",
      "Telcm      date\n",
      "1926-07-01   -1.501137\n",
      "1926-08-01    0.02...  \n",
      "Servs      date\n",
      "1926-07-01    6.196397\n",
      "1926-08-01   -0.77...  \n",
      "BusEq      date\n",
      "1926-07-01   -1.485065\n",
      "1926-08-01    1.15...  \n",
      "Paper      date\n",
      "1926-07-01    4.522559\n",
      "1926-08-01   -5.28...  \n",
      "Trans      date\n",
      "1926-07-01   -1.566586\n",
      "1926-08-01    1.71...  \n",
      "Whlsl      date\n",
      "1926-07-01   -27.074958\n",
      "1926-08-01     2....  \n",
      "Rtail      date\n",
      "1926-07-01   -3.126948\n",
      "1926-08-01   -3.66...  \n",
      "Meals      date\n",
      "1926-07-01   -1.312481\n",
      "1926-08-01   -3.03...  \n",
      "Fin        date\n",
      "1926-07-01   -3.284140\n",
      "1926-08-01    1.14...  \n",
      "Other      date\n",
      "1926-07-01    1.995716\n",
      "1926-08-01    3.86...  \n",
      "[0.65131797] 28.856116803781866\n",
      "GRS Statistic (W): 0.05373235192733061, Scaled GRS F-Statistic: 1.8877966310468821, p-value: 0.0028090166871348066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \\nJointly equal to zero means that there is no portfolio that can beat the market portfolio.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question B \n",
    "\n",
    "def estimate_regressions(x, y):\n",
    "    results = []\n",
    "    # **FIXED:** Use y.columns instead of the hardcoded vw_industries.columns \n",
    "    # to keep the function general if you reuse it.\n",
    "    for columns in y.columns:\n",
    "        X = sm.add_constant(x)\n",
    "        Y = y[columns]\n",
    "        # Data alignment should be handled in the call, but OLS handles indices automatically\n",
    "        model =sm.OLS(Y, X, missing='drop').fit()\n",
    "        alpha = model.params['const']\n",
    "        beta = model.params[x.columns[0]]\n",
    "        residuals = model.resid\n",
    "        results.append((columns, alpha, beta, residuals))\n",
    "    return pd.DataFrame(results, columns=['Portfolio', 'Alpha', 'Beta', 'Residuals']).set_index('Portfolio')\n",
    "\n",
    "# Use vw_industries (already excess returns) as the test assets (y)\n",
    "regression_results = estimate_regressions(factors[['RM-RF']], vw_industries)\n",
    "print(regression_results)\n",
    "\n",
    "# Careful need to check if this covariance matrix is unbiased or not \n",
    "def residuals_covariance_matrix(residuals):\n",
    "    # Stack residuals into a 2D array: shape (n_portfolios, n_periods) of dimension (n_portfolios x n_periods)\n",
    "    residuals_matrix = np.vstack([r.values for r in residuals.values]).T \n",
    "    # **FIXED:** Changed to rowvar=False for T rows, N columns to ensure correct N x N covariance \n",
    "    # calculation, which is the preferred way when data is organized T x N.\n",
    "    residuals_covariance_matrix = np.cov(residuals_matrix, rowvar=False)\n",
    "    return residuals_covariance_matrix\n",
    "\n",
    "sigma_hat = residuals_covariance_matrix(regression_results['Residuals'])\n",
    "\n",
    "# Now lets compute the sample means and sample covariance matrix \n",
    "def sample_means_and_covariance(returns):\n",
    "    returns_matrix = returns.values\n",
    "    # Calculates mean for each column (factor)\n",
    "    sample_means_vector = returns_matrix.mean(axis=0) \n",
    "    # Computes K x K covariance matrix (rowvar=False assumes T rows, K columns)\n",
    "    sample_means_covariance = np.cov(returns_matrix, rowvar=False)\n",
    "    return sample_means_vector, sample_means_covariance\n",
    "\n",
    "# **CORRECTION:** The call uses the correct factor data (RM-RF)\n",
    "sample_means, sample_covariance_matrix = sample_means_and_covariance(factors[['RM-RF']])\n",
    "print(sample_means, sample_covariance_matrix)\n",
    "\n",
    "def grs_statistic(T, N, K, alpha_vector, sigma_hat, m_hat, omega_hat):\n",
    "    \"\"\"\n",
    "    Computes the GRS test statistic. (Fixed scaling and input reshaping)\n",
    "\n",
    "    Args:\n",
    "        T (int): Number of time periods.\n",
    "        N (int): Number of test assets/portfolios.\n",
    "        K (int): Number of factors.\n",
    "        alpha_vector (np.ndarray): N x 1 vector of estimated intercepts (alphas).\n",
    "        sigma_hat (np.ndarray): N x N covariance matrix of residuals (Sigma).\n",
    "        m_hat (np.ndarray): K x 1 vector of factor means (m).\n",
    "        omega_hat (np.ndarray): K x K covariance matrix of factors (Omega).\n",
    "\n",
    "    Returns:\n",
    "        float: The GRS test statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are correctly shaped for matrix algebra\n",
    "    alpha_vector = alpha_vector.reshape(-1, 1)\n",
    "    m_hat = m_hat.reshape(-1, 1)\n",
    "    # If omega_hat is a scalar (K=1), ensure it's treated as a 1x1 array\n",
    "    if K == 1 and omega_hat.ndim == 0:\n",
    "        omega_hat = np.array([[omega_hat]])\n",
    "\n",
    "    # 1. Numerator: alpha' * Sigma_inv * alpha\n",
    "    numerator_quadratic = alpha_vector.T @ solve(sigma_hat, alpha_vector)\n",
    "    \n",
    "    # 2. Denominator: 1 + m' * Omega_inv * m\n",
    "    denominator_quadratic = m_hat.T @ solve(omega_hat, m_hat)\n",
    "    denominator = 1 + denominator_quadratic\n",
    "\n",
    "    # GRS Statistic (W)\n",
    "    grs_stat = (numerator_quadratic / denominator)\n",
    "    grs_stat_w = grs_stat[0][0]\n",
    "\n",
    "    # 3. Scaling factor (FIXED TO STANDARD GRS F-STATISTIC FORMULA)\n",
    "    df1 = N\n",
    "    df2 = T - N - K\n",
    "    # Standard GRS F-statistic scaling for F(N, T-N-K) distribution\n",
    "    scaling_factor_f = (T - N - K) / N\n",
    "    \n",
    "    grs_stat_normalised = grs_stat_w * scaling_factor_f\n",
    "\n",
    "    # 4. Compute the p-value\n",
    "    p_value = f.sf(grs_stat_normalised, df1, df2)\n",
    "\n",
    "    return grs_stat_w, grs_stat_normalised, p_value\n",
    "\n",
    "# Determine K based on the factor data used in the regression call\n",
    "K = factors[['RM-RF']].shape[1] \n",
    "\n",
    "# **CORRECTION:** Reshape K-dimensional factor moments to be safe for the function call\n",
    "m_hat_reshaped = sample_means.reshape(K, 1)\n",
    "omega_hat_reshaped = sample_covariance_matrix.reshape(K, K)\n",
    "\n",
    "# The shape attributes (T, N, K) are calculated correctly in Cell 1/start of Cell 3.\n",
    "grs_stat_w, grs_stat_f, p_value = grs_statistic(\n",
    "    T=vw_industries.shape[0], \n",
    "    N=vw_industries.shape[1], \n",
    "    K=K, \n",
    "    alpha_vector=regression_results['Alpha'].values, \n",
    "    sigma_hat=sigma_hat, # Now based on correct residuals covariance calculation\n",
    "    m_hat=m_hat_reshaped, \n",
    "    omega_hat=omega_hat_reshaped\n",
    ")\n",
    "\n",
    "print(f\"GRS Statistic (W): {grs_stat_w}, Scaled GRS F-Statistic: {grs_stat_f}, p-value: {p_value}\")\n",
    "\n",
    "''' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \n",
    "Jointly equal to zero means that there is no portfolio that can beat the market portfolio.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0081090-d732-46d5-a885-3fee7bc684ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portfolio</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loser</th>\n",
       "      <td>0.040287</td>\n",
       "      <td>9.807258</td>\n",
       "      <td>0.004108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.420964</td>\n",
       "      <td>8.065572</td>\n",
       "      <td>0.052193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.474041</td>\n",
       "      <td>6.995907</td>\n",
       "      <td>0.067760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599333</td>\n",
       "      <td>6.366311</td>\n",
       "      <td>0.094141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.604365</td>\n",
       "      <td>5.964666</td>\n",
       "      <td>0.101324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.657665</td>\n",
       "      <td>5.814916</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.736172</td>\n",
       "      <td>5.508820</td>\n",
       "      <td>0.133635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.839082</td>\n",
       "      <td>5.352057</td>\n",
       "      <td>0.156778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.920176</td>\n",
       "      <td>5.635042</td>\n",
       "      <td>0.163295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winner</th>\n",
       "      <td>1.225672</td>\n",
       "      <td>6.493693</td>\n",
       "      <td>0.188748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>0.651776</td>\n",
       "      <td>6.600424</td>\n",
       "      <td>0.098748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean       Std  Sharpe Ratio\n",
       "Portfolio                                  \n",
       "Loser      0.040287  9.807258      0.004108\n",
       "2          0.420964  8.065572      0.052193\n",
       "3          0.474041  6.995907      0.067760\n",
       "4          0.599333  6.366311      0.094141\n",
       "5          0.604365  5.964666      0.101324\n",
       "6          0.657665  5.814916      0.113100\n",
       "7          0.736172  5.508820      0.133635\n",
       "8          0.839082  5.352057      0.156778\n",
       "9          0.920176  5.635042      0.163295\n",
       "Winner     1.225672  6.493693      0.188748\n",
       "Overall    0.651776  6.600424      0.098748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Part 2\n",
    "## Question A \n",
    "''' Lets calculate the sample mean and sample standard deviation for each of the 10 portfolios.'''\n",
    "results = []\n",
    "\n",
    "def summary_stats(df):\n",
    "    for columns in df.columns: \n",
    "        mean = df[columns].mean()\n",
    "        std = df[columns].std()\n",
    "        sharpe_ratio = mean / std if std != 0 else 0\n",
    "        results.append((columns, mean, std, sharpe_ratio))\n",
    "    df_results = pd.DataFrame(results, columns= ['Portfolio', 'Mean', 'Std', 'Sharpe Ratio'])\n",
    "    df_results.set_index('Portfolio', inplace=True)\n",
    "    overall_mean = df_results['Mean'].mean()\n",
    "    overall_std = df_results['Std'].mean()\n",
    "    overall_sharpe = overall_mean / overall_std if overall_std != 0 else 0\n",
    "    df_results.loc['Overall'] = [overall_mean, overall_std, overall_sharpe]\n",
    "    return df_results\n",
    "\n",
    "summary_stats_pr = summary_stats(pr_portfolios)\n",
    "summary_stats_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73370fe-0b46-4fdf-905a-a8fa0a809be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Alpha      Beta  \\\n",
      "Portfolio                       \n",
      "Loser     -0.969041  1.558457   \n",
      "2         -0.439353  1.328375   \n",
      "3         -0.287373  1.175664   \n",
      "4         -0.108870  1.093502   \n",
      "5         -0.068067  1.038272   \n",
      "6         -0.010003  1.030915   \n",
      "7          0.108158  0.969687   \n",
      "8          0.234110  0.934110   \n",
      "9          0.295572  0.964422   \n",
      "Winner     0.562602  1.023815   \n",
      "\n",
      "                                                   Residuals  \n",
      "Portfolio                                                     \n",
      "Loser      date\n",
      "1927-01-01   -2.507452\n",
      "1927-02-01    1.68...  \n",
      "2          date\n",
      "1927-01-01   -4.350945\n",
      "1927-02-01    0.63...  \n",
      "3          date\n",
      "1927-01-01    2.767913\n",
      "1927-02-01    3.44...  \n",
      "4          date\n",
      "1927-01-01   -0.395520\n",
      "1927-02-01    2.78...  \n",
      "5          date\n",
      "1927-01-01   -0.529636\n",
      "1927-02-01   -0.73...  \n",
      "6          date\n",
      "1927-01-01    0.781858\n",
      "1927-02-01   -0.39...  \n",
      "7          date\n",
      "1927-01-01    0.470023\n",
      "1927-02-01   -1.61...  \n",
      "8          date\n",
      "1927-01-01   -0.068064\n",
      "1927-02-01   -1.11...  \n",
      "9          date\n",
      "1927-01-01   -0.917706\n",
      "1927-02-01   -0.37...  \n",
      "Winner     date\n",
      "1927-01-01   -0.991173\n",
      "1927-02-01    1.85...  \n",
      "[0.65131797] 28.856116803781866\n",
      "GRS Statistic (W): 0.06056449073165129, Scaled GRS F-Statistic: 6.504626304579348, p-value: 8.240681642972517e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \\nJointly equal to zero means that there is no portfolio that can beat the market portfolio.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question B \n",
    "\n",
    "def estimate_regressions(x, y):\n",
    "    results = []\n",
    "    # **FIXED:** Use y.columns instead of the hardcoded pr_portfolios.columns \n",
    "    # to keep the function general if you reuse it.\n",
    "    for columns in y.columns:\n",
    "        X = sm.add_constant(x)\n",
    "        Y = y[columns]\n",
    "        # Data alignment should be handled in the call, but OLS handles indices automatically\n",
    "        model =sm.OLS(Y, X, missing='drop').fit()\n",
    "        alpha = model.params['const']\n",
    "        beta = model.params[x.columns[0]]\n",
    "        residuals = model.resid\n",
    "        results.append((columns, alpha, beta, residuals))\n",
    "    return pd.DataFrame(results, columns=['Portfolio', 'Alpha', 'Beta', 'Residuals']).set_index('Portfolio')\n",
    "\n",
    "# Use pr_portfolios (already excess returns) as the test assets (y)\n",
    "regression_results = estimate_regressions(factors[['RM-RF']], pr_portfolios)\n",
    "print(regression_results)\n",
    "\n",
    "# Careful need to check if this covariance matrix is unbiased or not \n",
    "def residuals_covariance_matrix(residuals):\n",
    "    # Stack residuals into a 2D array: shape (n_portfolios, n_periods) of dimension (n_portfolios x n_periods)\n",
    "    residuals_matrix = np.vstack([r.values for r in residuals.values]).T \n",
    "    # **FIXED:** Changed to rowvar=False for T rows, N columns to ensure correct N x N covariance \n",
    "    # calculation, which is the preferred way when data is organized T x N.\n",
    "    residuals_covariance_matrix = np.cov(residuals_matrix, rowvar=False)\n",
    "    return residuals_covariance_matrix\n",
    "\n",
    "sigma_hat = residuals_covariance_matrix(regression_results['Residuals'])\n",
    "\n",
    "# Now lets compute the sample means and sample covariance matrix \n",
    "def sample_means_and_covariance(returns):\n",
    "    returns_matrix = returns.values\n",
    "    # Calculates mean for each column (factor)\n",
    "    sample_means_vector = returns_matrix.mean(axis=0) \n",
    "    # Computes K x K covariance matrix (rowvar=False assumes T rows, K columns)\n",
    "    sample_means_covariance = np.cov(returns_matrix, rowvar=False)\n",
    "    return sample_means_vector, sample_means_covariance\n",
    "\n",
    "# **CORRECTION:** The call uses the correct factor data (RM-RF)\n",
    "sample_means, sample_covariance_matrix = sample_means_and_covariance(factors[['RM-RF']])\n",
    "print(sample_means, sample_covariance_matrix)\n",
    "\n",
    "def grs_statistic(T, N, K, alpha_vector, sigma_hat, m_hat, omega_hat):\n",
    "    \"\"\"\n",
    "    Computes the GRS test statistic. (Fixed scaling and input reshaping)\n",
    "\n",
    "    Args:\n",
    "        T (int): Number of time periods.\n",
    "        N (int): Number of test assets/portfolios.\n",
    "        K (int): Number of factors.\n",
    "        alpha_vector (np.ndarray): N x 1 vector of estimated intercepts (alphas).\n",
    "        sigma_hat (np.ndarray): N x N covariance matrix of residuals (Sigma).\n",
    "        m_hat (np.ndarray): K x 1 vector of factor means (m).\n",
    "        omega_hat (np.ndarray): K x K covariance matrix of factors (Omega).\n",
    "\n",
    "    Returns:\n",
    "        float: The GRS test statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are correctly shaped for matrix algebra\n",
    "    alpha_vector = alpha_vector.reshape(-1, 1)\n",
    "    m_hat = m_hat.reshape(-1, 1)\n",
    "    # If omega_hat is a scalar (K=1), ensure it's treated as a 1x1 array\n",
    "    if K == 1 and omega_hat.ndim == 0:\n",
    "        omega_hat = np.array([[omega_hat]])\n",
    "\n",
    "    # 1. Numerator: alpha' * Sigma_inv * alpha\n",
    "    numerator_quadratic = alpha_vector.T @ solve(sigma_hat, alpha_vector)\n",
    "    \n",
    "    # 2. Denominator: 1 + m' * Omega_inv * m\n",
    "    denominator_quadratic = m_hat.T @ solve(omega_hat, m_hat)\n",
    "    denominator = 1 + denominator_quadratic\n",
    "\n",
    "    # GRS Statistic (W)\n",
    "    grs_stat = (numerator_quadratic / denominator)\n",
    "    grs_stat_w = grs_stat[0][0]\n",
    "\n",
    "    # 3. Scaling factor (FIXED TO STANDARD GRS F-STATISTIC FORMULA)\n",
    "    df1 = N\n",
    "    df2 = T - N - K\n",
    "    # Standard GRS F-statistic scaling for F(N, T-N-K) distribution\n",
    "    scaling_factor_f = (T - N - K) / N\n",
    "    \n",
    "    grs_stat_normalised = grs_stat_w * scaling_factor_f\n",
    "\n",
    "    # 4. Compute the p-value\n",
    "    p_value = f.sf(grs_stat_normalised, df1, df2)\n",
    "\n",
    "    return grs_stat_w, grs_stat_normalised, p_value\n",
    "\n",
    "# Determine K based on the factor data used in the regression call\n",
    "K = factors[['RM-RF']].shape[1] \n",
    "\n",
    "# **CORRECTION:** Reshape K-dimensional factor moments to be safe for the function call\n",
    "m_hat_reshaped = sample_means.reshape(K, 1)\n",
    "omega_hat_reshaped = sample_covariance_matrix.reshape(K, K)\n",
    "\n",
    "# The shape attributes (T, N, K) are calculated correctly in Cell 1/start of Cell 3.\n",
    "grs_stat_w, grs_stat_f, p_value = grs_statistic(\n",
    "    T=pr_portfolios.shape[0], \n",
    "    N=pr_portfolios.shape[1], \n",
    "    K=K, \n",
    "    alpha_vector=regression_results['Alpha'].values, \n",
    "    sigma_hat=sigma_hat, # Now based on correct residuals covariance calculation\n",
    "    m_hat=m_hat_reshaped, \n",
    "    omega_hat=omega_hat_reshaped\n",
    ")\n",
    "\n",
    "print(f\"GRS Statistic (W): {grs_stat_w}, Scaled GRS F-Statistic: {grs_stat_f}, p-value: {p_value}\")\n",
    "\n",
    "''' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \n",
    "Jointly equal to zero means that there is no portfolio that can beat the market portfolio.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65f548a-92be-4cd6-b80d-485ff816577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portfolio</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Small_Low</th>\n",
       "      <td>0.563687</td>\n",
       "      <td>12.266929</td>\n",
       "      <td>0.045952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small_2</th>\n",
       "      <td>0.691816</td>\n",
       "      <td>9.873443</td>\n",
       "      <td>0.070068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small_3</th>\n",
       "      <td>0.994387</td>\n",
       "      <td>9.026214</td>\n",
       "      <td>0.110167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small_4</th>\n",
       "      <td>1.183364</td>\n",
       "      <td>8.366578</td>\n",
       "      <td>0.141439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small_High</th>\n",
       "      <td>1.364378</td>\n",
       "      <td>9.319727</td>\n",
       "      <td>0.146397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_Low</th>\n",
       "      <td>0.621069</td>\n",
       "      <td>8.011713</td>\n",
       "      <td>0.077520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_2</th>\n",
       "      <td>0.926046</td>\n",
       "      <td>7.519225</td>\n",
       "      <td>0.123157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_3</th>\n",
       "      <td>0.990949</td>\n",
       "      <td>7.278665</td>\n",
       "      <td>0.136144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_4</th>\n",
       "      <td>1.079475</td>\n",
       "      <td>7.444352</td>\n",
       "      <td>0.145006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_High</th>\n",
       "      <td>1.256424</td>\n",
       "      <td>8.724975</td>\n",
       "      <td>0.144003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_Low</th>\n",
       "      <td>0.698682</td>\n",
       "      <td>7.429648</td>\n",
       "      <td>0.094040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2</th>\n",
       "      <td>0.906396</td>\n",
       "      <td>6.486081</td>\n",
       "      <td>0.139745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_3</th>\n",
       "      <td>0.933484</td>\n",
       "      <td>6.530463</td>\n",
       "      <td>0.142943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_4</th>\n",
       "      <td>1.018912</td>\n",
       "      <td>6.913129</td>\n",
       "      <td>0.147388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_High</th>\n",
       "      <td>1.158157</td>\n",
       "      <td>8.532615</td>\n",
       "      <td>0.135733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_Low</th>\n",
       "      <td>0.711005</td>\n",
       "      <td>6.274531</td>\n",
       "      <td>0.113316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_2</th>\n",
       "      <td>0.748184</td>\n",
       "      <td>6.094339</td>\n",
       "      <td>0.122767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_3</th>\n",
       "      <td>0.862369</td>\n",
       "      <td>6.433555</td>\n",
       "      <td>0.134042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_4</th>\n",
       "      <td>0.970221</td>\n",
       "      <td>6.821182</td>\n",
       "      <td>0.142237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_High</th>\n",
       "      <td>1.033696</td>\n",
       "      <td>8.739688</td>\n",
       "      <td>0.118276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big_Low</th>\n",
       "      <td>0.611152</td>\n",
       "      <td>5.356306</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big_2</th>\n",
       "      <td>0.626433</td>\n",
       "      <td>5.316426</td>\n",
       "      <td>0.117830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big_3</th>\n",
       "      <td>0.701991</td>\n",
       "      <td>5.654709</td>\n",
       "      <td>0.124143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big_4</th>\n",
       "      <td>0.651207</td>\n",
       "      <td>6.626669</td>\n",
       "      <td>0.098271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big_High</th>\n",
       "      <td>0.960267</td>\n",
       "      <td>8.577938</td>\n",
       "      <td>0.111946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>0.890550</td>\n",
       "      <td>7.584764</td>\n",
       "      <td>0.117413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Mean        Std  Sharpe Ratio\n",
       "Portfolio                                    \n",
       "Small_Low   0.563687  12.266929      0.045952\n",
       "Small_2     0.691816   9.873443      0.070068\n",
       "Small_3     0.994387   9.026214      0.110167\n",
       "Small_4     1.183364   8.366578      0.141439\n",
       "Small_High  1.364378   9.319727      0.146397\n",
       "2_Low       0.621069   8.011713      0.077520\n",
       "2_2         0.926046   7.519225      0.123157\n",
       "2_3         0.990949   7.278665      0.136144\n",
       "2_4         1.079475   7.444352      0.145006\n",
       "2_High      1.256424   8.724975      0.144003\n",
       "3_Low       0.698682   7.429648      0.094040\n",
       "3_2         0.906396   6.486081      0.139745\n",
       "3_3         0.933484   6.530463      0.142943\n",
       "3_4         1.018912   6.913129      0.147388\n",
       "3_High      1.158157   8.532615      0.135733\n",
       "4_Low       0.711005   6.274531      0.113316\n",
       "4_2         0.748184   6.094339      0.122767\n",
       "4_3         0.862369   6.433555      0.134042\n",
       "4_4         0.970221   6.821182      0.142237\n",
       "4_High      1.033696   8.739688      0.118276\n",
       "Big_Low     0.611152   5.356306      0.114100\n",
       "Big_2       0.626433   5.316426      0.117830\n",
       "Big_3       0.701991   5.654709      0.124143\n",
       "Big_4       0.651207   6.626669      0.098271\n",
       "Big_High    0.960267   8.577938      0.111946\n",
       "Overall     0.890550   7.584764      0.117413"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Part 3\n",
    "## Question A \n",
    "''' Lets calculate the sample mean and sample standard deviation for each of the 10 portfolios.'''\n",
    "results = []\n",
    "\n",
    "def summary_stats(df):\n",
    "    for columns in df.columns: \n",
    "        mean = df[columns].mean()\n",
    "        std = df[columns].std()\n",
    "        sharpe_ratio = mean / std if std != 0 else 0\n",
    "        results.append((columns, mean, std, sharpe_ratio))\n",
    "    df_results = pd.DataFrame(results, columns= ['Portfolio', 'Mean', 'Std', 'Sharpe Ratio'])\n",
    "    df_results.set_index('Portfolio', inplace=True)\n",
    "    overall_mean = df_results['Mean'].mean()\n",
    "    overall_std = df_results['Std'].mean()\n",
    "    overall_sharpe = overall_mean / overall_std if overall_std != 0 else 0\n",
    "    df_results.loc['Overall'] = [overall_mean, overall_std, overall_sharpe]\n",
    "    return df_results\n",
    "\n",
    "summary_stats_beme = summary_stats(beme_portfolios)\n",
    "summary_stats_beme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4113c931-e5bb-43b8-bed5-210954d9f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Alpha      Beta  \\\n",
      "Portfolio                        \n",
      "Small_Low  -0.498134  1.630265   \n",
      "Small_2    -0.225871  1.408970   \n",
      "Small_3     0.100179  1.372921   \n",
      "Small_4     0.356579  1.269402   \n",
      "Small_High  0.465675  1.379822   \n",
      "2_Low      -0.203407  1.265858   \n",
      "2_2         0.127288  1.226372   \n",
      "2_3         0.210850  1.197724   \n",
      "2_4         0.289742  1.212515   \n",
      "2_High      0.358482  1.378654   \n",
      "3_Low      -0.112276  1.245103   \n",
      "3_2         0.173301  1.125557   \n",
      "3_3         0.201535  1.123797   \n",
      "3_4         0.263645  1.159598   \n",
      "3_High      0.260708  1.377896   \n",
      "4_Low      -0.000194  1.091938   \n",
      "4_2         0.044986  1.079654   \n",
      "4_3         0.135973  1.115270   \n",
      "4_4         0.218534  1.154101   \n",
      "4_High      0.108564  1.420400   \n",
      "Big_Low    -0.011064  0.955319   \n",
      "Big_2       0.007790  0.949832   \n",
      "Big_3       0.071279  0.968363   \n",
      "Big_4      -0.070742  1.108444   \n",
      "Big_High    0.105792  1.311918   \n",
      "\n",
      "                                                    Residuals  \n",
      "Portfolio                                                      \n",
      "Small_Low   date\n",
      "1926-07-01    -0.767450\n",
      "1926-08-01    -6....  \n",
      "Small_2     date\n",
      "1926-07-01    -4.574678\n",
      "1926-08-01   -12....  \n",
      "Small_3     date\n",
      "1926-07-01   -6.324026\n",
      "1926-08-01   -1.53...  \n",
      "Small_4     date\n",
      "1926-07-01   -3.984011\n",
      "1926-08-01   -3.34...  \n",
      "Small_High  date\n",
      "1926-07-01   -2.719947\n",
      "1926-08-01    4.04...  \n",
      "2_Low       date\n",
      "1926-07-01   -1.573533\n",
      "1926-08-01   -1.21...  \n",
      "2_2         date\n",
      "1926-07-01   -1.557350\n",
      "1926-08-01   -4.79...  \n",
      "2_3         date\n",
      "1926-07-01   -3.486113\n",
      "1926-08-01    0.38...  \n",
      "2_4         date\n",
      "1926-07-01    -5.678787\n",
      "1926-08-01    -3....  \n",
      "2_High      date\n",
      "1926-07-01   -4.989299\n",
      "1926-08-01    1.92...  \n",
      "3_Low       date\n",
      "1926-07-01   -1.883230\n",
      "1926-08-01   -4.36...  \n",
      "3_2         date\n",
      "1926-07-01   -1.294949\n",
      "1926-08-01   -1.00...  \n",
      "3_3         date\n",
      "1926-07-01   -3.737974\n",
      "1926-08-01   -1.44...  \n",
      "3_4         date\n",
      "1926-07-01   -0.456056\n",
      "1926-08-01    0.87...  \n",
      "3_High      date\n",
      "1926-07-01    -5.819280\n",
      "1926-08-01     3....  \n",
      "4_Low       date\n",
      "1926-07-01   -1.861942\n",
      "1926-08-01   -1.80...  \n",
      "4_2         date\n",
      "1926-07-01   -1.930762\n",
      "1926-08-01    0.72...  \n",
      "4_3         date\n",
      "1926-07-01   -2.467174\n",
      "1926-08-01   -1.32...  \n",
      "4_4         date\n",
      "1926-07-01   -3.584675\n",
      "1926-08-01   -1.34...  \n",
      "4_High      date\n",
      "1926-07-01   -2.062948\n",
      "1926-08-01    1.23...  \n",
      "Big_Low     date\n",
      "1926-07-01    0.413321\n",
      "1926-08-01   -1.75...  \n",
      "Big_2       date\n",
      "1926-07-01    3.050706\n",
      "1926-08-01    1.42...  \n",
      "Big_3       date\n",
      "1926-07-01   -1.127632\n",
      "1926-08-01   -0.86...  \n",
      "Big_4       date\n",
      "1926-07-01   -0.320252\n",
      "1926-08-01    2.37...  \n",
      "Big_High    date\n",
      "1926-07-01   -3.649068\n",
      "1926-08-01    3.94...  \n",
      "[0.65131797] 28.856116803781866\n",
      "GRS Statistic (W): 0.08341834056107018, Scaled GRS F-Statistic: 3.533600906166933, p-value: 1.3894339206607048e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \\nJointly equal to zero means that there is no portfolio that can beat the market portfolio.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question B \n",
    "\n",
    "def estimate_regressions(x, y):\n",
    "    results = []\n",
    "    # **FIXED:** Use y.columns instead of the hardcoded beme_portfolios.columns \n",
    "    # to keep the function general if you reuse it.\n",
    "    for columns in y.columns:\n",
    "        X = sm.add_constant(x)\n",
    "        Y = y[columns]\n",
    "        # Data alignment should be handled in the call, but OLS handles indices automatically\n",
    "        model =sm.OLS(Y, X, missing='drop').fit()\n",
    "        alpha = model.params['const']\n",
    "        beta = model.params[x.columns[0]]\n",
    "        residuals = model.resid\n",
    "        results.append((columns, alpha, beta, residuals))\n",
    "    return pd.DataFrame(results, columns=['Portfolio', 'Alpha', 'Beta', 'Residuals']).set_index('Portfolio')\n",
    "\n",
    "# Use beme_portfolios (already excess returns) as the test assets (y)\n",
    "regression_results = estimate_regressions(factors[['RM-RF']], beme_portfolios)\n",
    "print(regression_results)\n",
    "\n",
    "# Careful need to check if this covariance matrix is unbiased or not \n",
    "def residuals_covariance_matrix(residuals):\n",
    "    # Stack residuals into a 2D array: shape (n_portfolios, n_periods) of dimension (n_portfolios x n_periods)\n",
    "    residuals_matrix = np.vstack([r.values for r in residuals.values]).T \n",
    "    # **FIXED:** Changed to rowvar=False for T rows, N columns to ensure correct N x N covariance \n",
    "    # calculation, which is the preferred way when data is organized T x N.\n",
    "    residuals_covariance_matrix = np.cov(residuals_matrix, rowvar=False)\n",
    "    return residuals_covariance_matrix\n",
    "\n",
    "sigma_hat = residuals_covariance_matrix(regression_results['Residuals'])\n",
    "\n",
    "# Now lets compute the sample means and sample covariance matrix \n",
    "def sample_means_and_covariance(returns):\n",
    "    returns_matrix = returns.values\n",
    "    # Calculates mean for each column (factor)\n",
    "    sample_means_vector = returns_matrix.mean(axis=0) \n",
    "    # Computes K x K covariance matrix (rowvar=False assumes T rows, K columns)\n",
    "    sample_means_covariance = np.cov(returns_matrix, rowvar=False)\n",
    "    return sample_means_vector, sample_means_covariance\n",
    "\n",
    "# **CORRECTION:** The call uses the correct factor data (RM-RF)\n",
    "sample_means, sample_covariance_matrix = sample_means_and_covariance(factors[['RM-RF']])\n",
    "print(sample_means, sample_covariance_matrix)\n",
    "\n",
    "def grs_statistic(T, N, K, alpha_vector, sigma_hat, m_hat, omega_hat):\n",
    "    \"\"\"\n",
    "    Computes the GRS test statistic. (Fixed scaling and input reshaping)\n",
    "\n",
    "    Args:\n",
    "        T (int): Number of time periods.\n",
    "        N (int): Number of test assets/portfolios.\n",
    "        K (int): Number of factors.\n",
    "        alpha_vector (np.ndarray): N x 1 vector of estimated intercepts (alphas).\n",
    "        sigma_hat (np.ndarray): N x N covariance matrix of residuals (Sigma).\n",
    "        m_hat (np.ndarray): K x 1 vector of factor means (m).\n",
    "        omega_hat (np.ndarray): K x K covariance matrix of factors (Omega).\n",
    "\n",
    "    Returns:\n",
    "        float: The GRS test statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are correctly shaped for matrix algebra\n",
    "    alpha_vector = alpha_vector.reshape(-1, 1)\n",
    "    m_hat = m_hat.reshape(-1, 1)\n",
    "    # If omega_hat is a scalar (K=1), ensure it's treated as a 1x1 array\n",
    "    if K == 1 and omega_hat.ndim == 0:\n",
    "        omega_hat = np.array([[omega_hat]])\n",
    "\n",
    "    # 1. Numerator: alpha' * Sigma_inv * alpha\n",
    "    numerator_quadratic = alpha_vector.T @ solve(sigma_hat, alpha_vector)\n",
    "    \n",
    "    # 2. Denominator: 1 + m' * Omega_inv * m\n",
    "    denominator_quadratic = m_hat.T @ solve(omega_hat, m_hat)\n",
    "    denominator = 1 + denominator_quadratic\n",
    "\n",
    "    # GRS Statistic (W)\n",
    "    grs_stat = (numerator_quadratic / denominator)\n",
    "    grs_stat_w = grs_stat[0][0]\n",
    "\n",
    "    # 3. Scaling factor (FIXED TO STANDARD GRS F-STATISTIC FORMULA)\n",
    "    df1 = N\n",
    "    df2 = T - N - K\n",
    "    # Standard GRS F-statistic scaling for F(N, T-N-K) distribution\n",
    "    scaling_factor_f = (T - N - K) / N\n",
    "    \n",
    "    grs_stat_normalised = grs_stat_w * scaling_factor_f\n",
    "\n",
    "    # 4. Compute the p-value\n",
    "    p_value = f.sf(grs_stat_normalised, df1, df2)\n",
    "\n",
    "    return grs_stat_w, grs_stat_normalised, p_value\n",
    "\n",
    "# Determine K based on the factor data used in the regression call\n",
    "K = factors[['RM-RF']].shape[1] \n",
    "\n",
    "# **CORRECTION:** Reshape K-dimensional factor moments to be safe for the function call\n",
    "m_hat_reshaped = sample_means.reshape(K, 1)\n",
    "omega_hat_reshaped = sample_covariance_matrix.reshape(K, K)\n",
    "\n",
    "# The shape attributes (T, N, K) are calculated correctly in Cell 1/start of Cell 3.\n",
    "grs_stat_w, grs_stat_f, p_value = grs_statistic(\n",
    "    T=beme_portfolios.shape[0], \n",
    "    N=beme_portfolios.shape[1], \n",
    "    K=K, \n",
    "    alpha_vector=regression_results['Alpha'].values, \n",
    "    sigma_hat=sigma_hat, # Now based on correct residuals covariance calculation\n",
    "    m_hat=m_hat_reshaped, \n",
    "    omega_hat=omega_hat_reshaped\n",
    ")\n",
    "\n",
    "print(f\"GRS Statistic (W): {grs_stat_w}, Scaled GRS F-Statistic: {grs_stat_f}, p-value: {p_value}\")\n",
    "\n",
    "''' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \n",
    "Jointly equal to zero means that there is no portfolio that can beat the market portfolio.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4344a77f-fcea-4378-95d3-44774579517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangency portfolio weights: [-0.45361504 -0.75250462 -0.47075843  0.81369172  0.88441869 -0.92607059\n",
      "  0.0870831   0.32691723  0.7300126   0.31204972 -0.54907905  0.99560097\n",
      "  0.35233356  0.36432601  0.13815683  0.97184336 -1.34094407 -0.10978097\n",
      "  0.47690186 -0.72340751  0.70746216 -0.14676221  0.71140198 -1.4998225\n",
      "  0.10054523]\n",
      "Sum of weights: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "## Question G\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract returns matrix\n",
    "R = beme_portfolios.values    # T x N, here N=25\n",
    "mu = R.mean(axis=0)           # vector of means (N,)\n",
    "Sigma = np.cov(R, rowvar=False, ddof=1)  # N x N covariance\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Step 2: Tangency portfolio weights\n",
    "ones = np.ones(mu.shape[0])\n",
    "Sigma_inv = inv(Sigma)\n",
    "w_unnormalized = Sigma_inv @ mu\n",
    "w_tan = w_unnormalized / (ones @ w_unnormalized)\n",
    "\n",
    "print(\"Tangency portfolio weights:\", w_tan)\n",
    "print(\"Sum of weights:\", w_tan.sum())  # should be 1\n",
    "\n",
    "# Step 3: Time-series of tangency portfolio returns\n",
    "R_tan = R @ w_tan   # T x 1 vector\n",
    "beme_portfolios['Tangency'] = R_tan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b30a246-5679-4fad-8d29-b1f38af0aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_tan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1926-07-01</th>\n",
       "      <td>-2.980133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926-08-01</th>\n",
       "      <td>9.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926-09-01</th>\n",
       "      <td>6.241085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926-10-01</th>\n",
       "      <td>10.379636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926-11-01</th>\n",
       "      <td>-3.793419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>2.420384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-01</th>\n",
       "      <td>4.563088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>-3.139405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-01</th>\n",
       "      <td>5.071848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>15.878986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1085 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                R_tan\n",
       "date                 \n",
       "1926-07-01  -2.980133\n",
       "1926-08-01   9.036278\n",
       "1926-09-01   6.241085\n",
       "1926-10-01  10.379636\n",
       "1926-11-01  -3.793419\n",
       "...               ...\n",
       "2016-07-01   2.420384\n",
       "2016-08-01   4.563088\n",
       "2016-09-01  -3.139405\n",
       "2016-10-01   5.071848\n",
       "2016-11-01  15.878986\n",
       "\n",
       "[1085 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tangency_factor_from_excess(df_excess: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df_excess: T x N DataFrame of *excess* returns (e.g., your beme_portfolios after subtracting RF).\n",
    "    Returns:\n",
    "        w_tan : (N,) ndarray of tangency weights (sum to 1)\n",
    "        R_tan : (T,) pandas Series of tangency (excess) returns\n",
    "    \"\"\"\n",
    "    # Drop rows with any NaNs to keep moments well-defined (optional: use a stricter alignment if needed)\n",
    "    R = df_excess.dropna(how='any').copy()\n",
    "    mu = R.mean(axis=0).values                # (N,)\n",
    "    Sigma = np.cov(R.values, rowvar=False, ddof=1)   # (N x N)\n",
    "\n",
    "    # Unconstrained tangency weights (shorting allowed)\n",
    "    # w*  ^{-1} , then normalize so that 1'w = 1\n",
    "    Sigma_inv_mu = solve(Sigma, mu)           # more stable than inv(Sigma) @ mu\n",
    "    denom = np.sum(Sigma_inv_mu)\n",
    "    w_tan = Sigma_inv_mu / denom              # (N,)\n",
    "\n",
    "    # Tangency returns time series: R_tan_t = w' R_t\n",
    "    R_tan_vals = R.values @ w_tan             # (T,)\n",
    "    R_tan = pd.Series(R_tan_vals, index=R.index, name='R_tan')\n",
    "\n",
    "    return w_tan, R_tan\n",
    "\n",
    "# --- Use it with your data (already excess returns) ---\n",
    "w_tan, R_tan = tangency_factor_from_excess(beme_portfolios)\n",
    "\n",
    "# If you want a factor DataFrame for regressions:\n",
    "tangency_factor = pd.DataFrame({'R_tan': R_tan})\n",
    "tangency_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01f06dd-b6eb-4c1a-97e0-50c2c6324af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Alpha      Beta  \\\n",
      "Portfolio                            \n",
      "Small_Low  -1.824937e-15  0.218377   \n",
      "Small_2    -1.834526e-15  0.268016   \n",
      "Small_3    -2.487096e-15  0.385235   \n",
      "Small_4    -4.048652e-15  0.458446   \n",
      "Small_High -3.201939e-15  0.528572   \n",
      "2_Low      -1.600737e-15  0.240608   \n",
      "2_2        -2.968010e-15  0.358759   \n",
      "2_3        -1.876356e-15  0.383903   \n",
      "2_4        -2.804954e-15  0.418198   \n",
      "2_High     -2.938578e-15  0.486750   \n",
      "3_Low      -1.058572e-15  0.270676   \n",
      "3_2        -3.118497e-15  0.351146   \n",
      "3_3        -2.101209e-15  0.361640   \n",
      "3_4        -3.412040e-15  0.394736   \n",
      "3_High     -3.497262e-15  0.448681   \n",
      "4_Low      -1.809675e-15  0.275450   \n",
      "4_2        -2.224929e-15  0.289853   \n",
      "4_3        -2.494938e-15  0.334090   \n",
      "4_4        -3.143915e-15  0.375873   \n",
      "4_High     -2.941337e-15  0.400463   \n",
      "Big_Low    -2.777792e-15  0.236766   \n",
      "Big_2      -2.024502e-15  0.242686   \n",
      "Big_3      -2.026828e-15  0.271958   \n",
      "Big_4      -2.160874e-15  0.252284   \n",
      "Big_High   -3.244200e-15  0.372016   \n",
      "Tangency   -7.916304e-15  1.000000   \n",
      "\n",
      "                                                    Residuals  \n",
      "Portfolio                                                      \n",
      "Small_Low   date\n",
      "1926-07-01     4.210794\n",
      "1926-08-01    -4....  \n",
      "Small_2     date\n",
      "1926-07-01     0.168723\n",
      "1926-08-01   -11....  \n",
      "Small_3     date\n",
      "1926-07-01   -1.011949\n",
      "1926-08-01   -1.29...  \n",
      "Small_4     date\n",
      "1926-07-01    1.496230\n",
      "1926-08-01   -3.78...  \n",
      "Small_High  date\n",
      "1926-07-01    3.405217\n",
      "1926-08-01    3.37...  \n",
      "2_Low       date\n",
      "1926-07-01     2.687044\n",
      "1926-08-01    -0....  \n",
      "2_2         date\n",
      "1926-07-01    3.269149\n",
      "1926-08-01   -4.67...  \n",
      "2_3         date\n",
      "1926-07-01    1.414082\n",
      "1926-08-01    0.29...  \n",
      "2_4         date\n",
      "1926-07-01    -0.553713\n",
      "1926-08-01    -3....  \n",
      "2_High      date\n",
      "1926-07-01    0.900580\n",
      "1926-08-01    1.52...  \n",
      "3_Low       date\n",
      "1926-07-01    2.496650\n",
      "1926-08-01   -3.63...  \n",
      "3_2         date\n",
      "1926-07-01    3.256463\n",
      "1926-08-01   -1.03...  \n",
      "3_3         date\n",
      "1926-07-01    0.867736\n",
      "1926-08-01   -1.54...  \n",
      "3_4         date\n",
      "1926-07-01    4.416366\n",
      "1926-08-01    0.63...  \n",
      "3_High      date\n",
      "1926-07-01    -0.142872\n",
      "1926-08-01     3....  \n",
      "4_Low       date\n",
      "1926-07-01    2.190877\n",
      "1926-08-01   -1.40...  \n",
      "4_2         date\n",
      "1926-07-01    2.173802\n",
      "1926-08-01    1.00...  \n",
      "4_3         date\n",
      "1926-07-01    1.965631\n",
      "1926-08-01   -1.25...  \n",
      "4_4         date\n",
      "1926-07-01    1.170150\n",
      "1926-08-01   -1.47...  \n",
      "4_High      date\n",
      "1926-07-01    3.443434\n",
      "1926-08-01    1.47...  \n",
      "Big_Low     date\n",
      "1926-07-01    3.935594\n",
      "1926-08-01   -1.37...  \n",
      "Big_2       date\n",
      "1926-07-01    6.593237\n",
      "1926-08-01    1.74...  \n",
      "Big_3       date\n",
      "1926-07-01    2.620470\n",
      "1926-08-01   -0.69...  \n",
      "Big_4       date\n",
      "1926-07-01    3.641839\n",
      "1926-08-01    2.95...  \n",
      "Big_High    date\n",
      "1926-07-01    1.448658\n",
      "1926-08-01    4.14...  \n",
      "Tangency    date\n",
      "1926-07-01    9.769963e-15\n",
      "1926-08-01    ...  \n",
      "[2.58125023] 67.62899932101367\n",
      "GRS Statistic (W): 0.0696828689789426, Scaled GRS F-Statistic: 2.835556745373895, p-value: 3.2978412909892397e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \\nJointly equal to zero means that there is no portfolio that can beat the market portfolio.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question G\n",
    "\n",
    "def estimate_regressions(x, y):\n",
    "    results = []\n",
    "    # **FIXED:** Use y.columns instead of the hardcoded beme_portfolios.columns \n",
    "    # to keep the function general if you reuse it.\n",
    "    for columns in y.columns:\n",
    "        X = sm.add_constant(x)\n",
    "        Y = y[columns]\n",
    "        # Data alignment should be handled in the call, but OLS handles indices automatically\n",
    "        model =sm.OLS(Y, X, missing='drop').fit()\n",
    "        alpha = model.params['const']\n",
    "        beta = model.params[x.columns[0]]\n",
    "        residuals = model.resid\n",
    "        results.append((columns, alpha, beta, residuals))\n",
    "    return pd.DataFrame(results, columns=['Portfolio', 'Alpha', 'Beta', 'Residuals']).set_index('Portfolio')\n",
    "\n",
    "# Use beme_portfolios (already excess returns) as the test assets (y)\n",
    "regression_results = estimate_regressions(tangency_factor, beme_portfolios)\n",
    "print(regression_results)\n",
    "\n",
    "# Careful need to check if this covariance matrix is unbiased or not \n",
    "def residuals_covariance_matrix(residuals):\n",
    "    # Stack residuals into a 2D array: shape (n_portfolios, n_periods) of dimension (n_portfolios x n_periods)\n",
    "    residuals_matrix = np.vstack([r.values for r in residuals.values]).T \n",
    "    # **FIXED:** Changed to rowvar=False for T rows, N columns to ensure correct N x N covariance \n",
    "    # calculation, which is the preferred way when data is organized T x N.\n",
    "    residuals_covariance_matrix = np.cov(residuals_matrix, rowvar=False)\n",
    "    return residuals_covariance_matrix\n",
    "\n",
    "sigma_hat = residuals_covariance_matrix(regression_results['Residuals'])\n",
    "\n",
    "# Now lets compute the sample means and sample covariance matrix \n",
    "def sample_means_and_covariance(returns):\n",
    "    returns_matrix = returns.values\n",
    "    # Calculates mean for each column (factor)\n",
    "    sample_means_vector = returns_matrix.mean(axis=0) \n",
    "    # Computes K x K covariance matrix (rowvar=False assumes T rows, K columns)\n",
    "    sample_means_covariance = np.cov(returns_matrix, rowvar=False)\n",
    "    return sample_means_vector, sample_means_covariance\n",
    "\n",
    "# **CORRECTION:** The call uses the correct factor data (RM-RF)\n",
    "sample_means, sample_covariance_matrix = sample_means_and_covariance(tangency_factor)\n",
    "print(sample_means, sample_covariance_matrix)\n",
    "\n",
    "def grs_statistic(T, N, K, alpha_vector, sigma_hat, m_hat, omega_hat):\n",
    "    \"\"\"\n",
    "    Computes the GRS test statistic. (Fixed scaling and input reshaping)\n",
    "\n",
    "    Args:\n",
    "        T (int): Number of time periods.\n",
    "        N (int): Number of test assets/portfolios.\n",
    "        K (int): Number of factors.\n",
    "        alpha_vector (np.ndarray): N x 1 vector of estimated intercepts (alphas).\n",
    "        sigma_hat (np.ndarray): N x N covariance matrix of residuals (Sigma).\n",
    "        m_hat (np.ndarray): K x 1 vector of factor means (m).\n",
    "        omega_hat (np.ndarray): K x K covariance matrix of factors (Omega).\n",
    "\n",
    "    Returns:\n",
    "        float: The GRS test statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are correctly shaped for matrix algebra\n",
    "    alpha_vector = alpha_vector.reshape(-1, 1)\n",
    "    m_hat = m_hat.reshape(-1, 1)\n",
    "    # If omega_hat is a scalar (K=1), ensure it's treated as a 1x1 array\n",
    "    if K == 1 and omega_hat.ndim == 0:\n",
    "        omega_hat = np.array([[omega_hat]])\n",
    "\n",
    "    # 1. Numerator: alpha' * Sigma_inv * alpha\n",
    "    numerator_quadratic = alpha_vector.T @ solve(sigma_hat, alpha_vector)\n",
    "    \n",
    "    # 2. Denominator: 1 + m' * Omega_inv * m\n",
    "    denominator_quadratic = m_hat.T @ solve(omega_hat, m_hat)\n",
    "    denominator = 1 + denominator_quadratic\n",
    "\n",
    "    # GRS Statistic (W)\n",
    "    grs_stat = (numerator_quadratic / denominator)\n",
    "    grs_stat_w = grs_stat[0][0]\n",
    "\n",
    "    # 3. Scaling factor (FIXED TO STANDARD GRS F-STATISTIC FORMULA)\n",
    "    df1 = N\n",
    "    df2 = T - N - K\n",
    "    # Standard GRS F-statistic scaling for F(N, T-N-K) distribution\n",
    "    scaling_factor_f = (T - N - K) / N\n",
    "    \n",
    "    grs_stat_normalised = grs_stat_w * scaling_factor_f\n",
    "\n",
    "    # 4. Compute the p-value\n",
    "    p_value = f.sf(grs_stat_normalised, df1, df2)\n",
    "\n",
    "    return grs_stat_w, grs_stat_normalised, p_value\n",
    "\n",
    "# Determine K based on the factor data used in the regression call\n",
    "K = tangency_factor.shape[1] \n",
    "\n",
    "# **CORRECTION:** Reshape K-dimensional factor moments to be safe for the function call\n",
    "m_hat_reshaped = sample_means.reshape(K, 1)\n",
    "omega_hat_reshaped = sample_covariance_matrix.reshape(K, K)\n",
    "\n",
    "# The shape attributes (T, N, K) are calculated correctly in Cell 1/start of Cell 3.\n",
    "grs_stat_w, grs_stat_f, p_value = grs_statistic(\n",
    "    T=beme_portfolios.shape[0], \n",
    "    N=beme_portfolios.shape[1], \n",
    "    K=K, \n",
    "    alpha_vector=regression_results['Alpha'].values, \n",
    "    sigma_hat=sigma_hat, # Now based on correct residuals covariance calculation\n",
    "    m_hat=m_hat_reshaped, \n",
    "    omega_hat=omega_hat_reshaped\n",
    ")\n",
    "\n",
    "print(f\"GRS Statistic (W): {grs_stat_w}, Scaled GRS F-Statistic: {grs_stat_f}, p-value: {p_value}\")\n",
    "\n",
    "''' basically the hypothesis test in a GRS tests to see if the alphas are jointly equal to zero. \n",
    "Jointly equal to zero means that there is no portfolio that can beat the market portfolio.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c573d5-0975-4937-9f34-f6f29dd23a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_from_A:\n",
      "Small_Low    -0.388619\n",
      "Small_2      -0.764958\n",
      "Small_3      -0.603358\n",
      "Small_4       1.247746\n",
      "Small_High    1.611316\n",
      "2_Low        -1.447250\n",
      "2_2          -0.166898\n",
      "2_3           0.596077\n",
      "2_4          -0.385811\n",
      "2_High       -0.259569\n",
      "3_Low        -0.341718\n",
      "3_2           0.690258\n",
      "3_3           0.430325\n",
      "3_4           1.292780\n",
      "3_High        0.434401\n",
      "4_Low         1.045363\n",
      "4_2          -1.712044\n",
      "4_3           0.681858\n",
      "4_4          -0.159061\n",
      "4_High       -0.880253\n",
      "Big_Low       0.505837\n",
      "Big_2         0.031639\n",
      "Big_3         0.601409\n",
      "Big_4        -1.218737\n",
      "Big_High      0.159266\n",
      "dtype: float64\n",
      "sum(w_from_A) = 1.0\n",
      "\n",
      "w_from_B:\n",
      "Small_Low    -0.507262\n",
      "Small_2      -0.859317\n",
      "Small_3      -0.406292\n",
      "Small_4       0.314666\n",
      "Small_High    0.590495\n",
      "2_Low        -0.541208\n",
      "2_2           0.619613\n",
      "2_3           0.167750\n",
      "2_4           1.573709\n",
      "2_High        0.781186\n",
      "3_Low        -0.573779\n",
      "3_2           1.109614\n",
      "3_3           0.156230\n",
      "3_4          -0.424105\n",
      "3_High       -0.237054\n",
      "4_Low         0.772666\n",
      "4_2          -1.384529\n",
      "4_3          -0.423558\n",
      "4_4           0.896843\n",
      "4_High       -0.581456\n",
      "Big_Low       1.267320\n",
      "Big_2        -0.459194\n",
      "Big_3         0.851603\n",
      "Big_4        -1.882774\n",
      "Big_High      0.178834\n",
      "dtype: float64\n",
      "sum(w_from_B) = 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import solve\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------------------------\n",
    "def split_interleaved(df: pd.DataFrame):\n",
    "    \"\"\"Return two interleaved halves A and B as described.\"\"\"\n",
    "    idx = df.index\n",
    "    even_year = (idx.year % 2 == 0)\n",
    "    odd_year  = ~even_year\n",
    "    even_mon  = (idx.month % 2 == 0)\n",
    "    odd_mon   = ~even_mon\n",
    "\n",
    "    # A: odd months in even years  OR  even months in odd years\n",
    "    mask_A = (even_year & odd_mon) | (odd_year & even_mon)\n",
    "    mask_B = ~mask_A\n",
    "\n",
    "    A = df.loc[mask_A].dropna(how='any')\n",
    "    B = df.loc[mask_B].dropna(how='any')\n",
    "    return A, B\n",
    "\n",
    "def tangency_weights(R_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Unconstrained tangency (max Sharpe) weights using excess returns.\n",
    "    w*  ^{-1} , normalized so 1'w = 1.\n",
    "    \"\"\"\n",
    "    mu = R_df.mean(axis=0).values                    # (N,)\n",
    "    Sigma = np.cov(R_df.values, rowvar=False, ddof=1)  # (N x N)\n",
    "    Sigma_inv_mu = solve(Sigma, mu)                  # more stable than inv\n",
    "    w = Sigma_inv_mu / Sigma_inv_mu.sum()\n",
    "    return w                                         # (N,)\n",
    "\n",
    "def apply_weights(R_df: pd.DataFrame, w: np.ndarray, name: str):\n",
    "    \"\"\"Portfolio returns series R_tan_t = w' R_t on the given sample.\"\"\"\n",
    "    vals = R_df.values @ w\n",
    "    return pd.Series(vals, index=R_df.index, name=name)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Prepare the 25 portfolio EXCESS returns (your beme_portfolios already are)\n",
    "# If you previously added a 'Tangency' column, drop it before computing w\n",
    "# -------------------------------------------------------------------\n",
    "R25 = beme_portfolios.copy()\n",
    "if 'Tangency' in R25.columns:\n",
    "    R25 = R25.drop(columns=['Tangency'])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# (a) Split into the two interleaved halves\n",
    "# -------------------------------------------------------------------\n",
    "R25_A, R25_B = split_interleaved(R25)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# (b) Estimate w on A, apply to B  (out-of-sample factor for B)\n",
    "# -------------------------------------------------------------------\n",
    "w_A = tangency_weights(R25_A)\n",
    "Rtan_B = apply_weights(R25_B, w_A, name='R_tan_from_A')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# (c) Estimate w on B, apply to A  (out-of-sample factor for A)\n",
    "# -------------------------------------------------------------------\n",
    "w_B = tangency_weights(R25_B)\n",
    "Rtan_A = apply_weights(R25_A, w_B, name='R_tan_from_B')\n",
    "\n",
    "# Optional: collect the two OOS tangency factors in one DataFrame\n",
    "tangency_oos = pd.concat([Rtan_A, Rtan_B], axis=1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example: run regressions OOS using your existing function\n",
    "#   (Use the factor that was built on the opposite half.)\n",
    "#   Here, as an example, regress the 25 portfolios OOS; you can swap in\n",
    "#   vw_industries (30 industries) or other test assets similarly.\n",
    "# -------------------------------------------------------------------\n",
    "# Regress on half A using factor built from B\n",
    "regressions_A = estimate_regressions(\n",
    "    x = tangency_oos[['R_tan_from_B']].loc[R25_A.index],\n",
    "    y = R25.loc[R25_A.index]\n",
    ")\n",
    "\n",
    "# Regress on half B using factor built from A\n",
    "regressions_B = estimate_regressions(\n",
    "    x = tangency_oos[['R_tan_from_A']].loc[R25_B.index],\n",
    "    y = R25.loc[R25_B.index]\n",
    ")\n",
    "\n",
    "# print weights estimated on A (used on B)\n",
    "print(\"w_from_A:\")\n",
    "print(pd.Series(w_A, index=R25.columns).round(6))\n",
    "print(\"sum(w_from_A) =\", float(w_A.sum()))\n",
    "\n",
    "# print weights estimated on B (used on A)\n",
    "print(\"\\nw_from_B:\")\n",
    "print(pd.Series(w_B, index=R25.columns).round(6))\n",
    "print(\"sum(w_from_B) =\", float(w_B.sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ebeb629-1176-41f7-b327-0e034743ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "1926-07-01    -6.629462\n",
      "1926-08-01    17.376519\n",
      "1926-09-01     1.231406\n",
      "1926-10-01    18.920053\n",
      "1926-11-01    -1.759664\n",
      "                ...    \n",
      "1934-06-01     7.862975\n",
      "1934-07-01    -9.327441\n",
      "1934-08-01     2.286055\n",
      "1934-09-01     5.907044\n",
      "1934-10-01    -3.481768\n",
      "Name: R_tan_full_oos, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Rtan_A = pd.Series(R25_A.values @ w_B, index=R25_A.index, name='R_tan_from_B')  # applied to A\n",
    "Rtan_B = pd.Series(R25_B.values @ w_A, index=R25_B.index, name='R_tan_from_A')  # applied to B\n",
    "Rtan_full_oos = pd.concat([Rtan_A, Rtan_B]).sort_index()\n",
    "Rtan_full_oos.name = 'R_tan_full_oos'\n",
    "\n",
    "print(Rtan_full_oos.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed30db3-f426-415b-87d8-ecedcc42963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.5 (v3.13.5:6cb20a219a8, Jun 11 2025, 12:23:45) [Clang 16.0.0 (clang-1600.0.26.6)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a178e-a075-49a9-8c1a-2c7926f85065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
